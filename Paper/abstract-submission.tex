Dealing with robots and other physical systems often leads to large amounts of complex, multimodal data, which may not be reliably available throughout the span of an interaction. Our study is motivated by needs in robotics, where an agent may have many sensors and thus many modalities by which a human may interact. Tying these various modalities together relates to a task in grounded language understanding in which natural language is used as a retrieval query against objects in a physical environment. However, current work has neglected the real-world constraint that sensors fail due to obstructions or adverse conditions. We propose a generalized distance-based loss function that can be used to learn retrieval models that incorporate an arbitrary number of views of a particular piece of data, compounded by the challenge of retrieval when a modality becomes unavailable. We demonstrate the usability of our model on a grounded language object retrieval scenario. We leverage four modalities including vision, depth sensing, text, and speech, and demonstrate that our model learns faster than state-of-the-art baselines, while outperforming them when less training data is available. The code is publicly available on GitHub and will be included for the camera-ready version.